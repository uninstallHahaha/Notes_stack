# Spider

> http://httpbin.org 一个测试http访问的网站





### python 自带的服务器工具

> python -m http.server 使用当前文件路径开启服务,8000端口



### 搭建代理服务器

1. linux系统

2. sudo apt install tinyproxy

3. vim /etc/tinyproxy/tinyproxy.conf

   将 Allow 127.0.0.1 这一行注释掉

   将 Port 这一行的端口号修改为随便一个端口

4. systemctl restart tinyproxy

5. 看看tinyproxy的日志  tail -f /var/log/tinyproxy/tinyproxy.log

6. 现在这个机器就开了一个代理服务







## 网页抓取

### 工具

#### ***curl*** 

>  命令行的http请求工具

> curl  http://www.baidu.com

-A "xxx" 设置请求时的 user-agent

> curl -A "Chrome"  http://www.baidu.com

-X xxx 设置请求方式

> curl -X POST  http://www.baidu.com

-I 只要返回的响应头

> curl -I  http://www.baidu.com

-d a=123 使用post方式发送请求, 并设置请求参数

> curl -d "a=1&b=2&c=3"  http://www.baidu.com
>
> 或者
>
> curl -d @参数文件路径  http://www.baidu.com
>
> ​	参数文件中写 a=1&b=2&c=3

-O 保存url返回的文件

> curl -O http://httpbin.org/mage/jpeg
>
> ​	这个url返回一个图片
>
> curl -o fox.jpeg http://httpbin.org/mage/jpeg 
>
> ​	设置自定义的保存文件名

-L 如果返回的是请求重定向, 直接再次请求转发地址并获取返回数据

> curl -L https://baidu.com

-H "xxx:xxx" 设置请求头信息

> curl -H "accept:image/webp" http://httpbin.org/image
>
> ​	这里设置请求头的 accept 参数

-k 允许请求没有 ssl证书的网站 (比如12306)(不安全的请求)

> curl -k https://12306.cn 不加这个参数就无法请求12306,因为12306没有安全证书

-b "xxx=xxx" 设置请求中的cookie参数

> curl -b a=1 http://httpbin.org/cookies

-s 不显示其他无关信息

-v 显示连接过程中的所有信息







#### ***wget***

> 一个专门用来下载的命令行工具

wget http://httpbin.org/image/png

-O xxx.xxx 以指定文件名下载

> wget -O test.png  http://httpbin.org/image/png

--limit-rate=20k 限速下载

> wget --limit-rate=20k https://qiniu-xpc0.xpccdn.com/5bd2809e89d9d.mp4

-c 使用端点续传, 如果中断下载,那么下一次再下载时会从上次的位置继续下载

> wget -c  https://qiniu-xpc0.xpccdn.com/5bd2809e89d9d.mp4

-b 后台下载, 会生成下载日志

> wget -bc  https://qiniu-xpc0.xpccdn.com/5bd2809e89d9d.mp4

-U "xxx" 设置 user-agent

> wget -U "Windows IE 10.0"  https://qiniu-xpc0.xpccdn.com/5bd2809e89d9d.mp4

--mirror  镜像复制该网站

-p 下载页面相关所有资源

>  wget --mirror -p http://docs.python-requests.org
>
> 结合使用就是克隆整个网站

-r 递归下载网站中所有的链接

> wget -r http://docs.python-requests.org

--convert-links 将下载的网站中的链接转换为本地链接

>  wget --mirror -p --convert-links http://docs.python-requests.org



#### httpie

> 一个发http请求的命令行工具

http http://httpbin.org/get



#### postman

> 一个模拟发送http请求的工具, 可以直接生成请求的各种语言的代码



#### charles

> 抓包工具

#### Fiddler

> 抓包工具





### python库

#### urllib

> python 标准库中发起 http 请求的一个包
>
> api 过于繁琐, 一点也不 pythonic , 浓浓的 java 风味

```python
import urllib.request
from urllib.request import urlopen

#直接发送一个 http 请求
r = urlopen('http://httpbin.org/get')
dir(r)
#读取 response内容, r 是 io 类型的对象
text = r.read()
print(text.decode('utf-8'))
# 将json 字符串转换为 对象
import json
json.loads(text)

#请求状态
r.status
r.reason
#头信息
dir(r.haeders)
dict(r.headers._headers)

#先修改请求头, 然后再发起请求
req = urllib.request.Request('http://httpbin.org/user-agent')
req.add_hearder('User-Agent','Chrome')
r = urllib.request.urlopen(req)
json.load(r)

#添加登录信息来登录需要验证登录信息的网站,realm参数随便写
handler = urllib.request.HTTPBasicAuthHandler()
handler.add_password(realm='xxx',
                    uri='/basic-auth/alice/123',
                    user='alice',
                    passwd='123')
opener = urllib.request.build_opener(handler)
urllib.request.install_opener(opener)
r = urllib.request.urlopen('http://httpbin.org')
print(r.read())

#添加 get 参数
params = urllib.parse.urlencode({'a':1, 'b':2, 'c':3})
url = 'http://httpbin.org/get?%s' % params
with urllib.request.urlopen(url) as r:
    print(json.load(r))
    
#添加 post 参数
data = urllib.parse.urlencode({'name':'alice', 'pass':'123'})
data = data.encode()
with urllib.request.urlopen('http://httpbin.org/post', data) as r:
    print(json.load(r))
    
#使用代理服务器发送 http 请求
proxy_handler = urllib.request.ProxyHandler({'http': '代理服务器的url地址'})
opener = urllib.request.build_opener(proxy_handler)
opener.open('http://httpbin.org/ip') #这个接口会返回请求ip
print(r.read())

#解析 url 地址中的各个组成部分, 能够解析任何协议的 url 地址字符串
o = urllib.parse.urlparse('http://alice:123@httpbin.org/get?name=alice#archer')
o.scheme
o.netloc
o.path
o.query
o.port
o.hostname
```



#### ***requests***

> python 工具包

```python
import requests

# get
r = requests.get('http://httpbin.org/get')
r = requests.get('http://httpbin.org/get' , params={'a':1, 'b':2})
print(r.status_code, r.reason)

# post
r = requests.post('http://httpbin.org/post', data={'name':'alice'})
print(r.json()) #将返回的内容按照json解析为对象


#设置请求头
headers = { 'User-Agent':'Chrome' }
r = requests.get('http://httpbin.org/headers', headers = headers)

#设置cookies
cookies = { 'name':'alice' }
r = requests.get('http://httpbin.org/cookies', cookies = cookies)

#设置登录认证信息
r = requests.get('http://httpbin.org/basic-auth/alice/123', auth = ('alice','123'))

#设置收到异常状态码时抛异常
bad_r = requests.get('http://httpbin.org/status/404')
bad_r.raise_for_status()

#使用 session 保存服务器返回的 cookies, 其实就是把浏览器返回的 cookies 保存到 s 这个对象中
#如果不使用 session 进行请求, 那么不会保存服务器返回的 cookies
s = requests.Session()
s.get('http://httpbin.org/cookies/set/name/alice') #这个地址会返回一个cookies
r = s.get('http://httpbin.org/cookies') #这个地址就是返回请求中的 cookies
r.json()

#使用代理
r = requests.get('http://httpbin.org/ip', proxies={'http':'代理服务器地址'}) 

#设置超时时间, 超时了会报错
r = requests.get('http://httpbin.org/delay/10', timeout=5) 
```





## 网页解析

##### bs4 : python

> pip install bs4
>
> 用来解析 xml 文档, html 属于 xml

```python
import bs4
from bs4 import BeautifulSoup

html_doc= '''
<html>
<head>
<title>title</title>
</head>
<body>
123
</body>
</html>
'''

#加载html 文档
soup = BeautifulSoup(html_doc)
#或者使用 lxml 的方式进行加载, 这种方式速度较快
lxml = BeautifulSoup(html_doc, 'lxml')

#美化html文档的格式
print(soup.prettify())

#取 title 标签及内容, 返回一个 title 对象, 可调用该对象中的各种属性
soup.title

#取出第一个 a 标签
soup.a

#取出所有的 a 标签
soup.find_all('a')

#取出所有属性
soup.a.attrs

#判断是否有某个属性
soup.a.has_attr('class')

#取子节点
soup.a.children

#根据某个属性为某个值来获取节点
soup.find(id='id1')

#css选择器
soup.select('.story') #类名为story的元素

#获取网页中的所有文字 (去除所有标签)
soup.get_text()
```



##### lxml : python

> pip install lxml

```python
from lxml import etree

html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""

#创建文档选择器
selector = etree.HTML(html_doc)
#根据 xpath 规则获取节点
links = selector.xpath('//p[@class="story"]/a/@href')
for i in links:
    print(i)

```

###### xpath

> 在 xml 中查找信息的语言

| 表达式                 | 描述                                         | 例子                                                         |
| ---------------------- | -------------------------------------------- | ------------------------------------------------------------ |
| 直接写节点的名字       | 按照节点名字选择                             |                                                              |
| //                     | 选择任意一层的子节点                         |                                                              |
| /                      | 从根选择 或者 代表下一级                     |                                                              |
| ./                     | 从当前节点同级选择                           |                                                              |
| ../                    | 从当前节点父级选择                           |                                                              |
| [@xxx="xxx"]           | 按照属性选择                                 |                                                              |
| @xxx                   | 获得某属性的值                               |                                                              |
| text()                 | 获得该节点中间夹着的文字                     |                                                              |
| 标签名[n]              | 选择这个标签列表中第n个元素 , 下标从 1 开始  |                                                              |
| 标签名[ last() ]       | 选择这个标签列表中 最后一个元素              |                                                              |
| 标签名[ last()-1 ]     | 选择这个标签列表中 倒数第二个元素            |                                                              |
| 标签名[ position()<3 ] | 选择这个标签列表中 前二个元素                |                                                              |
| 标签名[ 标签名>30 ]    | 根据这个标签的子标签中的 text 做条件判断选择 |                                                              |
| contains( a, b )       | 包含查询                                     | 标签名[ contains(@class, "book") ]<br/>选择class属性包含boos的标签 |



## 实例 : 抓取下厨房首页图片

```python
from bs4 import BeautifulSoup
import requests
import os
from urllib.parse import urlparse

#请求主页所有数据
r = requests.get('http://www.xiachufang.com/')
soup = BeautifulSoup(r.text)

#获取图片数据
img_list = []
for img in soup.select('img'):
    if img.has_attr('data-src'):
    	img_list.append(img.attrs['data-src'])
    else:
        img_list.append(img.attrs['src'])
        
#初始化保存图片的目录
img_dir = os.path.join(os.curdir, 'imgs')
if not os.path.isdir(img_dir):
    os.mkdir(img_dir)

    
#保存图片
for img in img_list:
    o = urlparse(img)
    file_path = os.path.join(img_dir, o.path[1:].split('@')[0])
    res = requests.get('%s://%s/%s' % (o.scheme, o.netloc, o.path[1:].split('@')[0]))
    with open(file_path, 'wb') as f:
        for chunk in res.iter_content(1024):
            f.write(chunk)
    
```



## Scrapy

> python 用来做爬虫的库

1. 创建爬虫文件 spider.py

```python
import scrapy

#继承 scrapy 的 Spider 类
class QuoteSpider(scrapy.Spider):
    name='quote'
    start_urls=['http://quotes.toscrape.com']
    
    #方法和参数是固定写法, 交给 scrapy 去调用
    def parse(self, response):
        quotes = response.xpath('//div[@class="quote"]')
        for q in quotes:
            yield {
                'text': q.css('span.text::text').extract_first(),
                'author': q.xpath('./span/small/text()').extract_first()
            }
        next_page = response.xpath('//li[@class="text"]/a/@href').extract_first()
        if next_page:
            yield response.follow(next_page, self.parse)
```

2. 在命令行使用  `scrapy runspider spider.py`