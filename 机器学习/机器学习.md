# **机器学习**

> sklearn 使用python实现的了各种机器学习基本算法的包

## 分类问题

### KNN

> k个邻近的数据作为一类的分类算法, 就是事先给出几类数据的分布, 然后对新的数据进行分类时, 根据这个新数据与原本就存在的聚类的距离给其分类, 离谁近就分到哪一类中
>
> 因为是跟每一个样本数据计算距离 , 所以如果样本数据的大小很影响运行效率

###### 使用knn的电影分类案例

```python
#导包
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier

#加载数据
#只加载第一个sheet中的内容
movie = pd.read_excel('./knn表.xls',sheet_name=0)
movie

#取出决定类型的数据
X = movie[['武打镜头','接吻镜头']]
X
#取出分类结果数据
y = movie['分类情况']
y

#初始化一个分类器 , 设置使用投票的数据数量为 5
knn = KNeighborsClassifier(n_neighbors = 5)

#传入数据和分类结果训练分类器
knn.fit(X,y)

#对新数据进行分类
X_test = pd.DataFrame({'武打镜头':[100], '接吻镜头':[3]})
X_test
#返回预测的分类结果
knn.predict(X_test)
#返回预测分类的概率
knn.predict_proba(X_test)

#原理 : 分别求新数据与原来数据的距离, 然后从小到大排列, 然后取前n_neighbors的数据, 使用它们的 分类情况 进行投票, 最后返回票数最多的那个分类
```

###### 使用knn进行手写文字的识别

> 首先准备数据:  
>
> 10 个文件夹, 文件夹命名分别为 0 ~ 9
>
> 每个文件夹下存放该文件夹对应数字的类型为 bmp类型的手写数据文件500个 , 命名为 x_xxx.bmp , 这些文件应当都是黑白图

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.neighbors import KNeignborsClassifier

#读取数据
data = []
for i in range(10):
    for j in range(1,501):
        data.append( plt.imread('./%d/%d_%d.bmp' % (i,i,j)) )
        
data
len(data)
X = np.array(data)

#创建初始的分类数据
#前500个是0, 500~1000个是1, 以此类推
y = [0,1,2,3,4,5,6,7,8,9]*500
y = np.array(y)
y.sort()
y

#随机选4000个数据作为训练模型使用的数据
#随机选1000个数据作为测试模型使用的数据
#此时 X_train 是三维数据 , 而knn只接受二维的样本数据, 所以要把数据转换为二维的, reshape时第二个参数为图片的宽*高, 也就是将图片的二维数据转换为一维数据, 因为knn只是计算距离, 所以样本数据的二维信息对它来说没有用
index = np.random.randint(0,5000,size=4000)
X_train = X[index]
X_train = X_train.reshape(4000, [图片的宽*高])
y_train = y[index]

index = np.random.randint(0,5000,size=1000)
X_test = X[index]
X_test = X_test.reshape(1000, [图片的宽*高])
y_test = y[index]

#使用样本训练模型
#初始化分类器时, 可以调整 n_neighbors 来优化分类结果, 也可以设置 weights 方式来优化
#设置 p 指定使用的距离计量方式
knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train, y_train)

#预测测试数据
y_ = knn.predict(X_test)
#对比预测结果和真实结果
y_[:20]
y_test[:20]
(y_ == y_test).mean()
knn.score(X_test, y_test)
```

###### knn准确率提升

```python
# 1. 调整分类器的参数

# 2. 归一化样本数据
# 如果样本数据各个属性的取值范围差距很大, 那么在求距离的过程中就相当于给这些属性加上了权重, 但是这并不是我们想要的, 因为每个属性的权重应当是一致的
#导包
from sklearn.preprocessing import StandardScaler
#获取归一化器
s = StandardScaler()
#归一化样本, X是样本数据
X2 = s.fit_transform(X)
X2
```



###### sklearn中的分割数据

> 将数据按照指定的比例随机分割为 训练数据和测试数据

```python
#导包
from sklearn.model_selection import train_test_split

#原数据
data = np.arange(100)

#分割数据
d_train, d_test = train_test_split(data, test_size=0.2)

#显示分割结果
display(d_train, d_test)
```

###### sklearn保存训练好的分类器

```python
#导包
from sklearn.externals import joblib

#存储模型为文件
joblib.dump(knn, './model')

#加载模型文件
model = joblib.load('./model')
model

#knn模型在保存时, 保存每一个样本点, 所以保存的文件较大
```

### Logistics 回归

> 虽然使用的是线性回归的方法, 求出的是线性函数, 但是该方法用于 **分类** 问题, 因为在其求出线性函数之后, 使用 sigmod 函数将函数的 y 的范围限制在 0~1 之间, 那么返回的就是各个分类的概率, 然后选择概率最大的分类作为结果范围,  使其变成一个处理分类问题的模型

```python
#导包
from sklearn.linear_model import LogisticRegression

#获取❀花分类问题的数据
import sklearn.datasets as datasets
iris = datasets.load_iris()
iris
#150个拥有4个属性的样本
X = iris['data']
y = iris['target']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y)
#使用Logistic模型训练数据
lg = LogisticRegression()
lg.fit(X_train, y_train)
y_ = lg.predict(X_test)
display(y_, y_test)
#查看各个分类的概率结果
lg.predict_proba(X_test)
#查看求解出的线性方程的系数
lg.coef_
```



## 回归问题

> 回归问题就是找出变量之间的函数关系, 然后使用这个函数对未来的值进行预测

### 总体方法 : 最小二乘法求函数

> 就是求 预测函数 和 真实函数 的差值的二次方的最小值, 这个值越小, 预测函数越接近真实函数

### 线性回归

#### 方法 : 矩阵运算求函数

> 假设样本有 n 个属性 , 那么线性回归求出来的函数就是 n 元一次方程 , 线性回归计算器返回的目标方程系数就有 n 个
>
> 换一个思路 : 这个求得的 n 个系数就是这 n 个属性的权重

```python
#导入sklearn 中自带的数据集
import sklearn.datasets as datasets

#获取波士顿房价数据 , 这个数据集属于回归问题数据集, 因为样本结果不是有限个分类, 而是无限个数字
data = datasets.load_boston()
data

#获取样本值
X = data['data']
X.shape

#获取样本对应的结果
y = data['target']
y.shape

#导入线性回归工具包
from sklearn.linear_model import LinearRegression

#分割数据为样本数据和测试数据
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)

#创建一个线性回归计算器 - 用来计算 x 和 y 之间的函数关系
#设置 fit_intercept = False , 设置计算出来的函数不包含截距, 也就是函数过原点
lr = LinearRegression(fit_intercept = False)

#使用线性回归计算器计算目标函数
lr.fit(X_train, y_train)

#预测测试数据的结果
lr.predict(X_test).round(2)

#对比真实结果
y_test

#查看求出的线性函数的参数, 这里样本数据有 13个属性,这个返回的参数列表就有 13 个元素
W_ = lr.coef_
w_

#查看结果函数的截距, 前面设置了不要截距, 那么返回的就是0
b_ = lr.intercept_
b_

#实际上 lr.predict() 就相当于系数矩阵 w_ 乘 测试的数据
X_test.dot(w_)
```

### Ridge 回归

#### 方法 : 梯度下降求函数最值

> **思路** : 在导数方向上进行固定步幅的移动, 越接近最值, x每次移动的值就越小( 因为导数会越来越小 ), 当x单次移动的值小于设定的值时, 即为近似的最值
>
> **实现** : 首先函数是一元二次函数, 
>
> ​	在函数上随机选择一个点, 设置导数方向每次移动的步幅 step, 设置一个精确度 p, 即当 x 单次移动量不超过这个值时, 终止移动
>
> ​	 然后无限循环, 每一次使得 x 移动导数方向上 step 距离对应的 x 的距离 , 直至 x 单次移动的值小于精确度 , 终止循环, 此时 x 即为近似的结果
>
> **关于 step** : 不能设置太大, 否则会跨过最值从而导致 x 无限增大
>
> 在sklearn的 LinearRegression 中, 如果设置了求截距, 那么它会使用梯度下降的方法求截距和参数.

> **拟合** : 使用线性回归的方法求解通用函数的系数的过程, 就称为 拟合
>
> **过拟合** : 求出的函数系数特别多, 或者系数特别大 , 那么函数不具有普遍适用性, 只有在数据的属性高度符合条件时, 才能给出相对正确的结果 , 就是学习特征的过程中学过头了.
>
> 换种方式理解 , 就是求出的函数顾虑了( 经过 )太多的样本点, 从而呈现出剧烈地不稳定性(百转千回), 而不是一条平滑的曲线或直线 , 这就使得系数们( 斜率 )变得很大,   那么在预测新数据时, 就不能很好的给出结果.
>
> **函数正则化** : 对于求解出来的函数 , 如果各个系数波动很大, 那么这个函数则不稳定.
>
> 正则化就是使得各个系数都在一个合适的范围内. 从而防止出现过拟合的情况 , 使得求出的函数具有普遍适应性.
>
> **Ridge回归就是使得求出函数的系数变小的线性回归方法**
>
> **Ridge回归是 线性回归的优化算法 , 是实现了L2正则化的线性回归算法**
>
> **L2正则化** : 在最小二乘法的损失函数基础上, 添加一个系数和的二次方 , 使用该二次方项来约束目标函数的系数使其变小

```python
#导包
import numpy as np
from sklearn.linear_model import LinearRegression, Ridge
import sklearn.datasets as datasets

#使用内置的波士顿房价数据
boston = datasets.load_bostom()
X = boston['data']
y = boston['target']

#切割数据
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)

#使用线性回归
lr = LinearRegression()
lr.fit(X_train, y_train)
display( lr.coef_, lr.score(X_test, y_test) )

#使用Ridge回归
#alpha 设置求解回归函数的方程中的 a 的值, 具体如何求解可在 sklearn 官网查看
#tol 设置精确度
#max_iter 设置最大迭代次数
ridge = Ridge(alpha=10)
ridge.fit(X_train, y_train)
display(ridge.coef_, ridge.score(X_test, y_test))

#对比两个回归方式得出的 coef_ , 可发现 Ridge 回归中的 coef_ 相对更接近于 0 
```

### Lasso 回归

> **Lasso回归是 线性回归的优化算法 , 是实现了L1正则化的线性回归算法**
>
> **L1正则化** : 在原最小二乘法损失函数的基础上, 添加了系数和的一次项, 使用这个一次项来约目标函数使其系数***归零***或减小

```python
#导包
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.linear_model import LinearRegression, Ridge, Lasso

#生成数据
#有200个属性的样本50个
X = np.random.randn(50,200)
X
#生成系数w
w = np.random.randn(200)
w
#随机选择190个系数使其为0
index = np.arange(200)
np.random.shuffle(index)
index
w[index[:190]] = 0
w
#生成目标值 y
y = X.dot(w)

#分别使用三种回归方式求系数
lr = LinearRegression(fit_intercept=False)
ridge = Ridge(alpha=1, fit_intercept=False)
lasso = Lasso(alpha=0.1, fit_intercept=False)

lr.fit(X,y)
ridge.fit(X,y)
lasso.fit(X,y)

#得到三种回归方式求得的系数
lr_w = lr.coef_
ridge_w = ridge.coef_
lasso_w = lasso.coef_

#将真实系数和三种方式求得的系数画图表示
plt.figure(figsize= (12,9))
ax = plt.subplot(2,2,1)
ax.plot(w)
ax.set_title('真实系数')

ax = plt.subplot(2,2,2)
ax.plot(lr_w)
ax.set_title('线性回归')

ax = plt.subplot(2,2,3)
ax.plot(ridge_w)
ax.set_title('ridge回归')

ax = plt.subplot(2,2,1)
ax.plot(lasso_w)
ax.set_title('lasso回归')

#对于该问题,因为真实系数属于稀松矩阵,大部分都是零, 而lasso回归将大部分系数都归为0, 所以在该问题中, lasso表现更好
```

### 回归算法人脸补全

```python
#导包
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
#使用knn求解线性函数的模型
from sklearn.neighbors import KNeighborsRegressor
import sklearn.datasets as datasets

#获取数据
faces = datasets.fetch_olivetti_faces()
faces
data = faces['images']
#400张64*64的图片
data.shape

#随机选择一张图片预览
index = np.random.randint(400,size=1)[0]
plt.imshow(data[index], cmap=plt.cm.gray)

#上下分割这400张图片, 上半部分作为 X 样本数据, 下半部分作为 y 目标数据
#然后将数据改成二维的方便之后训练
X = data[:,:32].reshape(400,-1)
y = data[:,32:].reshape(400,-1)

#分割数据为训练数据和测试数据
from sklearn.model_selection import train_test_split
#保留10条数据作为测试数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 10)
X_train.shape

#随机选择一张图片预览上下分割效果
index = np.random.randint(390,size=1)[0]
face_up = X_train[index].reshape(32,64)
face_down = y_train[index].reshape(32,64)
ax = plt.subplot(1,2,1)
ax.imshow(face_up, cmap=plt.cm.gray)
ax = plt.subplot(1,2,2)
ax.imshow(face_down, cmap=plt.cm.gray)

#分别使用四种回归算法进行函数的计算
es  = {}
es['knn'] = KNeighborsRegressor(n_neighbors=5)
es['lr'] = LinearRegression()
es['ridge'] = Ridge(alpha=1)
es['lasso'] = Lasso(alpha=1)
predict_ = {}
for key, model in es.items():
	model.fit(X_train, y_train)
	y_ = model.predict(X_test)
	predict_[key] = y_
    
#显示各种算法预测的结果 10行6列
plt.figure(figsize=(6*2, 10*2))
for i in range(10):
    #第一列 , 真实的整个人脸
    ax = plt.subplot(10, 6, 1+i*6)
    face_up = X_test[i].reshape(32,64)
    face_down = y_test[i].reshape(32,64)
    ax.imshow(np.concatenate([face_up, face_down], axis=0),cmap='gray')
    ax.axis('off')
    ax.set_title('true')
    
    #第二列 , 上半个人脸
    ax = plt.subplot(10, 6, 2+i*6)
    face_up = X_test[i].reshape(32,64)
    ax.imshow(face_up ,cmap='gray')
    ax.axis('off')
    ax.set_title('face_up')

    #3,4,5,6列, 各种算法预测的下半张人脸
    for j,key in enumerate(predict_):
        ax = plt.subplot(10, 6, 3+j+i*6)
        y_ = predict_[key]
        face_down_ = y_[i].reshape(32,64)
        ax.imshow(np.concatenate([face_up, face_down_], axis=0), cmap = plt.cm.gray)
        ax.axis('off')
        ax.set_title(key)
        
```



## 聚类问题

#### K-means 聚类

> 1. 打算把一堆点分成 k 类
> 2. 先随机选 k 个点 , 将这 k 个点周围离它相对近的点分为一类
> 3. 将生成的几个类别中的重心点作为这一类的中心点, 再次按照周围离它较近的点划分为一类
> 4. 循环以上两步最终将得到分类

![1614045798114](机器学习.assets/1614045798114.png)

足球队梯队聚类

```python
#上图为使用到的数据集 , 读取到 data 变量中
#使用 k-means 对足球队水平进行聚类
from sklearn.cluster import KMeans

X = data.iloc[:,1:]
#创建聚类算法
#n_cluster 设置分为几类
kmeans = KMeans(n_cluster=3)
#fit数据, 没有标签, 不需要y
kmeans.fit(X)
#查看聚类结果
y_ = kmeans.predict(X)
y_
#分为三类, 分别是 0, 1, 2
for i in range(3):
    print((data['国家'].values)[np.argwhere(y_==i).ravel()])
```

```python
import sklearn.datasets as datasets

#生成三个聚类的每个元素包含两个特征值且总数为100个的样本点
X,y = datasets.make_blobs()
plt.scatter(X[:,0], X[:,1], c=y)

#使用kmeans进行聚类, 如果设置的分类数不符合样本数据本身的分类情况, 那么多次执行聚类会发现其分类结果都不相同
kmeans = KMeans(5)
kmeans.fit(X)
y_ = kmeans.predict(X)
plt.scatter(X[:,0], X[:,1], c=y_)

#使用轮廓系数来评价聚类得分, 分越高越好
from sklearn.metrics import silhouette_score
silhouette_score(X, y_)
```







## 决策树:分类问题

### 单个决策树

> **决策树** : 对于一个通过判别多个条件后可以将样本完成分类的问题 , 可以构建成一个决策树. 就是每次按照一个条件去分类, 直至最后将所有的数据都分类完成.
>
> **信息熵** : 使用信息熵衡量一个系统中信息的有序程度, 这个值越大 , 系统的信息越无序 
>
> ***那么如何构建这个决策树, 才能使得每次决策进行得更加高效 ?***
>
> ​	使用 香农的 **信息传送速率公式** 可计算出不同结构的决策树对信息整理的高效程度, 通过计算不同决策树的信息传送速率, 比较之后选择速率最高的方式, 即为最高效的决策树结构
>
> sklearn中使用的决策树为 CART 方式, 即二叉树.
>
> **决策树的生成** : 使用指定的标准分别计算以各个属性作为树的第一层时拥有最大信息增益的情况, 使用这种情况构建决策树的第一层, 以此类推, 直至将所有属性都安排为树的枝干 , 此时即为一颗完成的决策树. 

```python
#导包
import numpy as np
from sklearn.tree import DecisionTreeClassifier
import sklearn.datasets as datasets

#获取数据
iris = datasets.load_iris()
X = iris['data']
y = iris['target']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#使用决策树分类器分类
#criterion 设置生成决策树使用的标准, entropy为信息熵
#max_depth 设置生成决策树的最大深度
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
clf.score(X_test, y_test)

#绘制决策树图形
#需要事先安装 pip install graphviz
#需要事先安装 graphviz-2.38.msi 这个软件, 软件安装完要配置环境变量
import graphviz
from sklearn import tree
data = tree.export_graphviz(clf, out_file=None,
                           feature_names = iris.feature_names,
                           class_names = iris.target_names,
                           filled = True)
graph = graphviz.Source(data)
graph
#保存决策树为pdf
graph.render('./iris')
#从文件中读取决策树
with open('./iris.pdf') as fp:
    dot = fp.read()
graphgiz.Source(dot)
```

> 单颗决策树进行分类可能效果不佳 , 实际生产中并不会使用单个决策树, 而是使用多个决策树共同决策的森林.

### 决策树森林

> **随机决策树森林** : 随机选取训练数据的多个子集训练生成多个决策树, 在进行分类预测时多个树同时给出结果, 然后选取这些结果中数量最多的分类作为结果输出.
>
> **极度随机决策树森林** : 在随机决策树森林的基础上, 把树中每个节点的分类值( 阈值 )改为随机生成并选取信息增益最大的分类值( 阈值 ). 

```python
#随机决策树森林
from sklearn.ensemble import RandomForestClassifier

#构建随机森林分类器
#n_estimators 指定使用多少个决策树, 默认10个
forest = RandomForestClassifier(n_estimators=20)
#训练和预测
forest.fit(X_train, y_train)
forest.score(X_test, y_test)

#极度随机决策树森林
from sklearn.ensemble import ExtraTreesClassifier
extra = ExtraTreesClassifier()
extra.fit(X_train, y_train)
extra.score(X_test, y_test)
```

### AdaBoost决策树

> 首先设置样本数据中每个属性的权重都相同, 生成第一个决策树, 然后多次迭代生成决策树, 每次迭代都根据上一次的分类结果将属性权重进行调整 ,  最后将这些决策树线性组合成为一个最后的决策树作为分类器输出

```python
from sklearn.ensemble import AdaBoostClassifier
#from sklearn.model_selectin import cross_val_score

ada = AdaBoostClassifier()
ada.fit(X_train, y_train)
ada.score(X_test, y_test)
```

### 梯度提升决策树 : 分类/回归

> 可以处理分类问题, 也可处理回归问题
>
> 构建多个根据不同属性进行分类的深度为1的决策树, 每个树中的每一种分支对应一个值, 处理回归问题时, 将数据遍历经过每一个树的到的结果加起来返回为预测值.

```python
#处理回归问题
from sklearn.ensemble import GradienBoostingClassifier
from sklearn.ensemble import GradienBoostingRegressor
from pandas import Series, DataFrame

#准备数据, 两个属性, 使用这两个属性回归计算用户的年龄 
X = DataFrame({'购物':[500,500,1500,1500], '是否百度回答': [0,1,0,1]})
y = np.array([14,16,24,26])

#使用梯度提升决策树回归
gbdt = GradienBoostingClassifier()
gbdt.fit(X,y)
#生成的决策树们
gbdt.estimators_
```

### XGBoost 决策树

> 对梯度提升决策树的改进
>
> 可处理分类问题和回归问题
>
> 使用该算法需要 pip install XGBoost
>
> 使用方法同 sklearn 中各种分类器的使用

### LightGBM 决策树

> 微软开发的, 对梯度提升决策树的改进
>
> 使用该算法需要 pip install lightgbm
>
> 使用方法同 sklearn 中各种分类器的使用

## 朴素贝叶斯:分类问题

> 贝叶斯公式 : 求在一个指定事件B发生的前提下, 另外的一个事件A 或一组事件A1~An 发生的概率的公式
>
> sklearn 中对贝叶斯公式采用了 独立性假设, 
>
> ​	即事件组 A1~An 都是独立发生的, 
>
> ​	此时 P(A1,...,An) = P(A1)* ... *P(An) , P(A1,...,An | B) = P(A1|B) * ... * P(An|B)

### 使用贝叶斯对三种数据模型分类

1. #### 正态分布 ( 高斯情况 )

2. #### 二分布( 投硬币 ) ( 伯努利情况 )

3. #### 多均匀分布( 掷骰子 )

```python
import matplotlib.pyplot as plt
%matplotlib inline

#导包
from sklearn.naive_bayes import GaussionNB, MultinomialNB, BernoulliNB

#使用skl 自带的数据
import sklearn.datasets as datasets
iris = datasets.load_iris()
X = iris['data']
y = iris['target']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)

#通过画图可知该数据集大致符合正态分布
plt.hist(X[:,0], bin=30)

#使用GaussionNB进行分类
gnb = GaussionNB()
gnb.fit(X_train, y_train)
gnb.score(X_test, y_test)

#使用MultinomialNB分类
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
mnb.score(X_test, y_test)

#使用MultinomialNB分类
bnb = BernoulliNB()
bnb.fit(X_train, y_train)
bnb.score(X_test, y_test)
#因为数据并不符合二分布, 所以使用这种方法分类结果很差
```



## 自然语言处理

#### sklearn中的词频统计

```python
#导包
from sklearn.feature_extraction.text import CountVectorizer

#创建词频统计器
#可设置参数 stop_words = 'english' , 统计时会自动忽略内置的英语中某些没用的词
cv = CountVectorizer()
#统计词频, 这里的 X 是自然语言文本的数组类型 , 
#返回一个 形状是 a*b的稀松矩阵, 其中a是句子的个数, b是所有句子中非重复单词的个数
#这个返回的稀松矩阵可作为以上分类问题中的样本数据, 然后配合文本的标签就可以训练给文本分类的模型
X_cv = cv.fit_transform(X)
#查看统计结果
print(X_cv)
#该算法将各个单词都映射成一个数字 , 可通过属性查看映射关系
cv.vocabulary_
```

#### sklearn - nlp - 新闻分类案例

```python
#导包
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
import sklearn.datasets as datasets

#新闻数据
#设置参数只要标签和正文部分
data = datasets.fetch_20newsgroups(remove=('headers','footers','quotes'))

#新闻标签
y = data.target
#新闻正文
X = data.data

#文本数据单词量化
from sklearn.feature_extration.text import CountVectorizer,TfidVectorizer

cv = CountVectorizer(stop_words='english')
X2 = cv.fit_transform(X)
X2

#分割数据为训练数据和测试数据
X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2)

#使用多分类分类器进行训练
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
mnb.score(X_test, y_test)

#使用二分类分类器进行训练, 因为新闻的分类数不是二分类问题, 所以这种方式效果很差
bnb = BernoulliNB()
bnb.fit(X_train, y_train)
bnb.score(X_test, y_test)

#对文本数据进行带权重的量化
tfidf = TfidfVectorizer(stop_words='english')
X3 = tfidf.fit_transform(X)
X3
#分割数据
X_train, X_test, y_train, y_test = train_test_split(X3, y, test_size=0.2)
#使用多分类分类器进行训练 , 结果发现带权重的单词量化分类结果更好
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
mnb.score(X_test, y_test)

#对文本数据进行考虑词组的量化
#ngram_range 指定考虑词组的长度
#因为考虑了词组的划分, 所以这样返回的单词频次矩阵会成倍增大
cv = CountVectorizer(stop_words='english', ngram_range=(1,3))
X4 = cv.fit_transform(X)
X4
```



#### nltk

> 一个独立的专门用来处理自然语言的工具包
>
> pip install nltk
>
> 官网需要翻墙



#### gensim 家的 word2vec

> pip install gensim



#### jieba : 中文处理工具

> pip install jieba
>
> 在处理中文时要注意两个 包含相同单词 但是语序不同 的句子 表达的意思不相同 的情况, 此时如果单纯按照单个单词进行向量化而不考虑词组 , 那么两个意义不同的句子返回的特征矩阵将会是相同的, 所以为了区分意义, 应当在单词向量化时考虑词组.





## SVM

### SVM - 回归问题

#### SVM处理回归问题 - 多项式回归

> 使用sklearn中的 svr 算法

> 线性问题就是求出来的是一条直线
>
> svr 算法求多项式时, 对二次幂方程表现不佳, 对更高次幂表现尚可
>
> 如果数据本身符合多项式的分布, 那么使用多项式算出来的函数能够较好地拟合过去或将来的数据

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
#导包
from sklearn.svm import SCR

#生成训练数据
X = np.linspace(-5,5,30).reshape(-1,1)
y = (X-2)**2 + X*5 + 12
#绘制真实的数据图形
plt.scatter(X,y)

#使用svm进行回归计算
# kernel 设置求得的回归函数的类型, poly是多项式, 即一元多次方程
# degree 设置多项式中最高的次幂
svr = SVR(kernel = 'ploy', degree=2)
svr.fit(X,y)
y_ = svr.predict(X)
#绘制预测的函数图形
plt.plot(X,y_)


#使用数据预处理将原本多次幂方程中一维的 X 数据变为二维数据
#从而可以使得 线性回归算法可以使用预处理后生成的数据对多次幂方程进行回归计算
from sklearn.preprocessing import PolynomialFeatures
#自变量数据转换
# degree 设置将原来的数据进行最高扩展为几次幂
poly = PolynomialFeature(degree = 3)
X_train = poly.fit_transform(X)
X_train.shape
#使用转换后的数据进行训练
lr = LinearRegression()
lr.fit(X_train,y)
#因为现在用于训练的样本集形状已经改变, 所以测试集也要使用相同形状的数据
X_test = poly.fit_transform(np.linspace(-10,10,100).reshape(-1,1))
y_ = lr.predict(X_test)
#此时便实现了使用线性回归解决非线性方程的计算问题
```

#### SVM处理回归问题 - 基于半径回归

> 如果数据能够被基于半径的回归算法拟合, 那么也仅仅是在样本范围内能较好地拟合, 出了样本范围就不能很好地拟合, 参照如下 对 sin 函数的回归

```python
#准备一个sin的数据(非线性)
X = np.linspace(0, 3*np.pi, 50).reshape(-1,1) 
y = np.sin(X)
plt.scatter(X,y)

#分别使用不同的算法对数据进行fit
svr_linear = SVR(kernel='linear')
svr_rbf = SVR(kernel='rbf')
svr_poly = SVR(kernel='poly')
svr_linear.fit(X,y)
svr_rbf.fit(X,y)
svr_poly.fit(X,y)

#准备测试数据
X_test = np.linspace(0, 3*np.pi, 180).reshape(-1,1)

#使用不同的算法进行测试
y1 = svr_linear.predict(X_test)
y2 = svr_rbf.predict(X_test)
y3 = svr_poly.predict(X_test)
#画图比较, 发现高斯分布的rbf算法能够对sin函数进行拟合, 但是超出样本的范围后, 预测函数就放飞自我了
plt.scatter(X,y)
plt.plot(X_test, y1)
plt.plot(X_test, y2)
plt.plot(X_test, y3)
```



### SVM - 分类问题

#### svm处理分类问题 - 线性划分

> svm分类使用 sklearn中的svc算法

> 此类问题中, 二维坐标系中的点可直接使用一条直线进行分类

```python
from sklearn.svm import SVC
import sklearn.datasets as datasets

#在坐标系中 生成一堆随机散布在 指定的中心点周围 的点
#n_sample 指定生成多少个点
#center 设置有几个中心点
#n_features 设置使用的是几维的坐标系, 也就是生成的点是几维的数据
#返回的 X 是样本点, y 是类别标签,y的种类数就是指定的center数
X, y = datasets.make_blobs(n_samples=50, center=2)
X.shape
y

#画图展示样本点
#c 指定点的颜色 , 如果给的是一个数组 , 那么按照数组中的数据的分类情况给点指定不同的颜色
#cmap 指定点颜色的映射值
from matplotlib.colors import ListedColormap
colors = ListedColormap(['r','b'])
plt.scatter(X[:,0], X[:,1], c=y, cmap=colors)

#创建一个svm分类器, 指定分类数据的类型为linear线性的, 默认是高斯分布
svc = SVC(kernel = 'linear')
#分类
svc.fit(X,y)
#获取到计算得出的系数和截距, 因为样本数据X有两个属性, 所以这里系数有两个
w1,w2 = svc.coef_[0]
b, = svc.intercept_
#此时求得的分割线应当为 y=x1*w1 + x2*w2 + b, 将样本点X的数据依次代入这个方程, 得到的y的值就是该点距离这条分割线的距离
#当上面这个方程的 y 等于 0 时, 即 0=x1*w1 + x2*w2 + b, 就是这条分割线
#其中 x1 和 x2 分别是 样本数据X中每一项的 x 的值和 y 的值
#那么就得到了这条直线在当前这个坐标系中的方程 , 即 0=w1*x + w2*y +b , 就是 y = (-w1/w2)*x - b/w2
#根据如上将分割线的斜率和截距求出
w_ = -w1/w2
b_ = -b/w2
#画出这条分割线
x = np.linspace(-2,2,50)
plt.plot(x, x*w_+b_)

#查看支持向量点
support_vectors_ = svc.support_vectors_
#画出支持向量点
plt.scatter(support_vectors_[:,0], support_vectors_[:,1], color='purple', s=300, alpha=0.3)
#画出过支持向量点的线
#就是画出距离分割线距离为 1 和 -1 的线
# 1=w1*x + w2*y +b 和 -1=w1*x + w2*y +b
# 由此可求出两条线各自的截距
b1 = -(b+1) / w2
b2 = -(b-1) / w2
#画线
plt.plot(x, x*w_+b1, ls='--')
plt.plot(x, x*w_+b2, ls='--')
```



#### svm处理分类问题 - 超平面划分 

> 此类问题中,二维坐标系中的点不可使用一条线进行划分, 所以需要将这些点映射到三维空间中, 使用一个平面划分

```python
#生成状态分布的300个二维的点
X = np.random.randn(300,2)
plt.scatter(X[:,0],X[:,1])

#使用点的坐标相乘, 如果大于0那么这个点就在一三象限, 反之则在二四象限
#使用该条件作为绘制图形颜色的分类, 可将点分为两类进行绘制
x = X[:,0]
y = X[:,1]
z = x*y
cond = z>=0
plt.scatter(x,y, c=cond)

#使用svm对点进行象限上的分类, 一三象限为一类, 二四象限为一类
#设置kernel为rbf指定数据分布情况为高斯分布
#样本中的分类数据则为上一步求出来的是否大于0的bool数组
svc = SVC(kernel='rbf')
svc.fit(X, cond)

#生成测试数据
x1 = np.linspace(-3,3,100)
y1 = np.linspace(-3,3,100)
X1, Y1 = np.meshgrid(x1, y1)
X_test = np.concatenate([X1.reshape(-1,1), Y1.reshape(-1,1)], axis=-1)
plt.scatter(X_test[:,0], X_test[:,1])

#使用svm进行分类
y_ = svc.predict(X_test)
plt.scatter(X_test[:,0], X_test[:,1], c=y_)

#查看各个点距离分类超平面的距离
d_ = svc.decision_function(X_test)
d_

#绘制各个点距离分类平面的等高线图
#在图中颜色越深离分离平面越远
plt.contourf(X1, Y1, d_.reshape(100,100))
```



### SVM 人脸识别案例 - 分类问题

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.svm import SVC

#获取人脸数据的训练集
#min_faces_per_person 设置每个人至少70张图片才列入到训练集中
#返回的是字典类型数据
faces = datasets.fetch_lfw_people(min_faces_per_person=70,resize=1)
faces

#获取样本数据, X就是图片每个像素的数据
X = faces['data']
y = faces['target']
names = faces.target_names
#1k多张人脸图片数据
images = faces['images']
images.shape

#随机查看一张图片
index = np.random.randint(1288,size=1)[0]
plt.imshow(image[index], cmap=plt.cm.gray)
names[y[index]]

#分割数据
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)

#创建svm分类器fit数据
svc = SVC(kernel='rbf')
svc.fit(X_train, y_train)
svc.score(X_test, y_test)

#然后发现效果很差, 因为样本数据中的特征过多, 有1w多个
#使用PCA主成分分析法对样本数据特征进行降维, 降维后的数据能够更好地反映数据的特征
from sklearn.decomposition import PCA
#n_components 设置将特征数据降维至多少维, 也就是保留多少个特征, 也可以设置百分比, 就是保留重要性前xx%的特征
#whiten 设置数据是否归一化, 归一化能对预测结果准确率有很大的提升
pca = PCA(n_components=0.9, whiten=True)
X_pca = pca.fit_transform(X)
#此时发现返回的结果数据中只保留了设置的特征数
X_pca.shape

#使用pca处理后的数据进行训练, 然后发现准确率大大提升
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2)
svc = SVC(kernel='rbf')
svc.fit(X_train, y_train)
svc.score(X_test, y_test)

y
#此时输出 y 发现训练样本中包含7个人 , 也就是将样本分为 7 类, 但是这七类的样本数据数量并不均衡, 这可能会影响模型的准确率 
#使用 imblearn 下的对样本均衡化
#需要 pip install imblearn
#如果下载之后无法使用, 可以手动下载这个包然后复制到 Anaconda > Lib > site-packages 中
from imblearn.over_sampling import SMOTE
#将原数据过采样, 也就是增加数据使其各个分类的样本数据平衡
X2, y2 = smote.fit_resample(X,y)
#使用均衡后的样本进行fit ,发现准确率又会有所提升
pca = PCA(n_components=0.9, whiten=True)
X2_pca = pca.fit_transform(X2)
#因为降维后的数据没法画图, 所以这里将原图数据也切分保留
face_train, face_test, X_train, X_test, y_train, y_test = 
	train_test_split(X2, X2_pca, y2, test_size=0.2)
svc = SVC(kernel='rbf')
svc.fit(X_train, y_train)
svc.score(X_test, y_test)
#预测的结果数据
y_ = svc.predict(X_test)
#将测试数据前100张原图画出来, 其中模糊的图片就是过采样造出来的数据
for i in range(100):
    ax = plt.subplot(10,10,i+1)
    face = face_test[i].reshape(127,94) #一维数据无法画图, 需要转换
    ax.imshow(face, cmap='gray')
    ax.axis('off')
    t = names[y_test[i]] #真实值
    p = names[y_[i]] #预测值
    ax.set_title('True: %s\nPredict: %s' % (t,p)) #对比真实值和预测值
```



# 特征工程

https://www.cnblogs.com/jasonfreak/p/5448385.html

## 数据预处理

#### PCA -- 数据降维

> 主成分分析, 用于对数据降维

```python
import numpy as np
import sklearn.datasets as datasets
from sklearn.decomposition import PCA

iris = datasets.load_iris()
X = iris['data']
y = iris['target']

#PCA降维
pca = PCA(n_components=0.95,whiten=False)
X_pca = pca.fit_transform(X)
X_pca

#PCA的计算原理
#1. 去中心化 , 就是每个元素都减去其所在列的平均值
A = X - X.mean(axis=0)
#2. 求协方差
V = np.cov(A, rowvar = False)
#3. 特征值和特征向量
T,Tv = np.linalg.eig(V)
#4. 选取大于95%的特征值所对应的的特征向量
cond = (T.cumsum()/T.sum()) > 0.95
P = Tv.loc[:, cond]
#5. 使用选取出来的特征向量和原始数据进行矩阵运算 , 这个结果就是上面pca求出来的结果
X.dot(P)
```

#### 特征筛选

> 去除原数据中 方差较小的特征属性

```python
#导包
from sklearn.feature_selection import VarianceThreshold
import sklearn.datasets as datasets

iris = datasets.load_iris()
X = iris['data']
X
X.var()
#特征方差筛选器, 经过方差筛选, 部分特征属性会被筛选去除, 留下特征的方差大于指定的值
v = VarianceThreshold(3)
X2 = v.fit_transform(X)
X2
X2.var()
```





# 参数交叉验证

> 在使用以上 sklearn 中的机器学习算法时, 很多时候需要通过多次对多个参数进行调整来获取更好的训练结果
>
> 这个过程就很繁琐且无聊
>
> 使用 GridSearchCV 方法可以一次性交叉尝试很多参数组合然后一次性返回所有参数下的训练结果

```python
#导包
from sklearn.model_selection import GridSearchCV

#以 svc 为例使用 GridSearchCV, 其中C和tol是SVC中的可调参数
C = [1,2,3]
tol = [1e-4, 1e-3, 1e-2]
svc = SVC()
clf = GridSearchCV(svc, param_grid={'C': C, 'tol':tol})
clf.fit(X_train, y_train)
#查看得分最高的参数组合, 此时直接使用 clf 进行预测使用的就是最佳参数的 SVC
clf.best_params_
clf.score(X_test, y_test)
#获取得分最高的 SVC 实例
clf.best_estimator_
```







# 附录

#### 回归问题

回归问题的本质是使用函数来拟合数据和结果之间的对应关系

衡量一个函数模型的好坏有两个标准 :  bias 和 various 

* bias 指该模型预测的结果整体与真实值之间的差异, 如果模型过于简单, 此时会造成 bias 过大, 也就是完全不能反映数据集的分布特点, 称之为欠拟合
* various 指该模型的各个预测点之间的差异, 如果模型过于复杂, 此时函数图像会跌宕起伏, 微小的自变量变化也会引起结果的剧烈变化, 且此时对测试集表现出极大的偏差, 称之为过拟合

解决欠拟合和过拟合?

欠拟合 : 适当增加参数的个数, 或者提高函数的复杂性,即增大函数的最大次幂

过拟合 : 使用更大的训练数据集能够很好的约束该复杂模型, 如果受限于训练数据集, 那么也可以尝试添加正则化参数, 正则化项会减缓原函数的跌宕起伏



#### 分类问题

分类问题的本质是根据 gaussion distribution ( 正态分布 ) 来算测试数据为某一类的可能性,  然后根据概率公式求出可能为该类的概率

步骤:

1. 使用给定的训练数据集, 每一个分类下有一批数据, 根据这批数据求出该分类的 gaussion distribution 函数, 也即是函数图像上的一个圆
2. 然后使用该 gaussion distribution 函数计算测试数据
3. 离圆心越近的为该分类的概率越大
4. 使用概率公式求出该分类的概率

