##### 分布式系统的CAP理论

C: consistency 一致性, 要求各个节点每时每刻都保持数据一致

A: Avalibility 可用性, 系统要求一直能够提供正常有效的服务

P: Partition Tolerance 分区容错性, 某个节点出现错误时, 整个系统仍然能对外正常提供服务

***P*** 是一定要满足的, 因为如果某个节点除了故障系统就瘫痪, 那么整这个分布式还有什么意义, 还不如直接单机运行

而且, CAP不同三个同时满足, 因此就必须从 C 或者 A 中舍弃一点





#### zookeeper 实操

​	用于存放分布式系统中的共享数据, 它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用 配置项的管理等

​	zookeeper满足的是CP, 不能确保总是可以正常访问, 但是保证数据绝对给你整对了

​	zookeeper学术上来说是 ***分布式协调系统*** , 以集群的方式给分布式系统提供统一的数据服务, 而且要从外边看像是一台单机, 它的优势就在于能够很快地完成数据的同步, 作为一个成熟的程序, 其他框架也就不自己再造轮子, 直接就使用zk, 因此zk才会流行起来





###### 读写分离?

​	分为leader, fllower, observer节点, 只有leader执行写操作, 其他节点提供读服务

###### session?

​	客户端与服务端建立连接时创建session, 定时心跳检测连接状态, 并且客户端可以watch节点变换, 当对应节点发生变化时, 服务端通知客户端, 然后客户端再读取新的值

###### 临时与永久节点?

​	可以选择创建临时或者永久节点, 临时节点在断开session连接时就自动删除

###### 节点存储?

​	数据节点按照文件的组织方式存储, /xxx/yyy/zzz

###### 数据发布与订阅?

​	发布者在指定节点添加信息, 订阅者watch这些节点实时获取到最新信息

###### 命名管理?

​	可以用作分布式命名管理系统, 使用自增值的节点作为分布式数据库记录的id, 可保证分布式数据库系统id不重复

###### 监听机制?

​	与zookeeper建立连接的会话可以选择监听感兴趣的节点的事件, 当发生这些事件时, 会话收到通知

###### 分布式锁?

​	请求临界数据时向zookeeper中添加临时编号节点, 然后检查是否为当前最小节点, 是则认为拿到了锁, 否则监听前一个编号节点的删除事件, 直至收到通知, 才代表获得了锁

###### 注册中心?

​	生产者在上面注册节点, 消费者从上面查看可用节点进行访问

###### 配置中心?

​	将分布式服务要用到的公共配置放到其节点上, 各个服务从上面取配置使用, 这样在配置需要修改时只需要修改zookeeper上的节点值即可





###### 分布式paxos算法

​	<span style='color:cyan;'>用于保证分布式系统中, 各个节点的数据一致性问题</span>

​	在该算法中, 分为 提议者, 接收者, 学习者 三种角色

***提议者***: 其实就是客户端, 用来向分布式系统中发送数据

***接收者***: 其实就是分布式系统的节点, 它们不仅要接收提议者的数据, 还要保证每个节点的数据都一致

***学习者***: 用来做数据冗余, 可以看作是分布式系统中的 slave

​	因为要保证所有节点的数据一致, 所以提议者每次有新的数据时都要向所有的接收者发送一份.

​	首先假设接收者一旦接收到提议者的数据就保存下来, 如果此时有多个提议者在发送数据, 而且存在网络波动, 那么它们的数据到达的时间将是乱序的, 那么肯定会造成各个接收者上最后保存下来的数据不尽相同, 来自于各个提议者, 此时分布式系统就处于数据不一致的境地.

​	paxos算法, 为了解决分布式系统各个节点的数据一致性问题

​	提议者向接收者写数据的过程分为两个阶段, ***准备*** 和 ***接收***

​	<span style='color:cyan;'>准备阶段</span>

​	各个提议者都产生一个编号发给各个接收者, 各个接收者都选最大的编号给予响应, 因为编号越大代表数据越新

​	<span style='color:cyan;'>接收阶段</span>

​	发送了小编号的提议者自然没有收到接收者的响应, 那么瞬间就明白了自己的数据不是最新的, 也就是本次写数据失败. 

​	而对于发送了最大编号的提议者, 它将会收到大部分接收者的响应, 也就是收到大部分节点说 "大, 你这个大, 我们存你的数", 然后这个最大编号提议者就把自己的值发送给所有的接收者 

​	接收者一看, 是这个最大编号对应的值, 就保存下来, 此时所有提议者节点都保存了最新的值 , 它们的数据是一致的 

​	最后学习者也默默地备份下这个值, 如果此时接收者中少数还没更新到最新值, 就由学习者发给它最新的值保存下来.

​	paxos算法能够保持所有节点数据一致性的本质在于, 第一次的响应是用来确定最后要用哪个值, 然后没说上话的提议者一看不是自己就不再写数据了, 第二阶段就只剩下被选中的提议者, 相当于它干掉了其他提议者, 最后把自己的值写到各个节点.

​	分成两个步骤的目的就是单独用第一个步骤确定最后要的, 到第二步时才真正进行写数据



###### zab协议

***写操作***

​	leader 节点用来执行写操作,  fllower节点用来执行读操作, 客户端随便连接一个节点, 如果是读操作那么直接进行, 如果是写操作那么转发给leader节点执行.

​	leader节点首先将写操作广播给其他fllower节点, fllower 节点收到消息后开始写事务, 然后返回给 leader 节点确认写消息, leader 收到大部分确认写操作后, 认定为本次写操作成功, 也就是 "我的小弟们都同意了"

​	然后再次广播提交写事务, 各个 fllower 收到后就提交写事务, 完成本次写操作的数据更新

***leader崩溃时***

​	选举新的leader , 然后其他 fllower 与新 leader 进行数据同步, 当大部分节点都完成了数据同步时, 认为系统恢复正常

***保证消息有序***

​	每个在集群中发送的广播都会带上一个编号zxid, 节点接收到编号后按照编号顺序执行









#### zookeeper 使用指南

> kafka 使用 zookeeper 作为服务注册发现中心

> 树形结构存储数据, 所有节点的路径都是 /xxx/xxx

##### 安装和启动

1. 下载并解压
2. 在目录下创建 data 文件夹, 用于存放数据
3. 复制 conf/zoo_sample.cfg 文件为 zoo.cfg 并修改 数据存放位置为上步新建的 data 文件夹
4. 在cmd中执行 bin 中的 zkServe.cmd

##### 使用zkCli.sh操作

> 执行 bin 中的 zkCli.cmd 默认连接本机2181端口
>
> 连接其他机器上的zookeeper时可设置 -server 参数来连接
>
> quit 退出

##### 列出节点

```shell
ls [路径]
```

##### 查看节点值

```shell
#无论节点是否有子节点, 它都能有值
get [节点路径]
```

##### 查看节点属性

```shell
stat [节点路径]
```

| **状态属性**   | **说明**                                                     |
| -------------- | ------------------------------------------------------------ |
| cZxid          | 数据节点创建时的事务ID                                       |
| ctime          | 数据节点创建时的时间                                         |
| mZxid          | 数据节点最后一次更新时的事务ID                               |
| mtime          | 数据节点最后一次更新时的时间                                 |
| pZxid          | 数据节点的子节点列表最后一次被修改（是子节点列表变更，而不是子节点内容变更）时的事务ID |
| cversion       | 子节点的版本号                                               |
| dataVersion    | 数据节点的版本号                                             |
| aclVersion     | 数据节点的ACL版本号                                          |
| ephemeralOwner | 如果节点是临时节点，则表示创建该节点的会话的SessionID；如果节点是持久节点，则该属性值为0 |
| dataLength     | 数据内容的长度                                               |
| numChildren    | 数据节点当前的子节点个数                                     |

##### 增加节点

有序节点做分布数据库id

> 有序节点可以作为分布式数据库数据的主键, 具体做法是首先规定一个有序节点名称作为数据库主键序号的生成节点, 每个客户端在往数据库存数据之前, 新增该有序节点, 因为zookeeper会自动在该节点后面加上唯一且递增的序号, 所以可以使用该序号作为要插入数据库数据的主键, 就保证了分库情况下数据主键的唯一.
>
> ![1606983591233](zookeeper.assets/1606983591233-1631955632228.png)
>
> 由图可知, 不断添加相同名称的有序节点, 后面的序号会逐个递增且不重复

有序节点做分布式锁

> 同样是使用有序节点, 创建 /Locks/Lock_ 的临时有序节点, 节点叫什么无所谓
>
> 对带锁保护的内容进行操作前, 先判断当前拿到的这个 Lock_节点序号的前一个节点是否存在, 不存在就代表轮到你了, 可以操作, 反之就代表被被人正操作着, 此时应当监听前一个节点的删除事件, 当其删除时, 就代表轮到自己了

使用方法

```shell
#创建持久节点, 不赋值默认为null
create [节点路径] [节点值]
#创建有序节点, 生成的节点名会后接序号
create -s [节点路径]
#创建临时节点, 该节点仅在本次shell中有效
create -e [节点路径]
#创建子节点, 首先得保证存在父节点, 如此时存在 node 节点, 为其创建子节点
create /node/second [值]
```

##### 修改节点

```shell
#普通修改节点
set [节点路径] [值]
#匹配版本号才能修改节点, 节点每修改一次, 版本号加一, 使用这种方式修改节点, 只有当前节点版本号等于给定版本号时, 才能修改成功
set [节点路径] [值] [版本号]
```

##### 删除节点

```shell
#只能删除没有子节点的节点
delete [节点路径]
#删除有子节点的节点
deleteall [节点路径]
```

##### 添加节点值监听

> 监听是一次性的, 捕获到一次动作后就失效 

> 这个可以做分布式配置共享, 将配置项设置为 节点值, 然后监听这些节点, 当收到变动消息时更新配置值

```shell
#先在一个 zkCli 中设置监听
get -s -w [节点路径]
#或者用 stat 添加监听也行, 没区别
stat -w [节点路径]
```

```shell
#再开一个 zkCli 修改监听的节点
set [节点路径] [值]
```

然后就会在添加监听的cmd中收到监听消息

![1606914092472](zookeeper.assets/1606914092472-1631955632229.png)

##### 添加节点监听

> 监听是一次性的, 捕获到一次动作后就失效

> 这个能监听该节点的子节点个数变化, 节点值变化不会监听

```shell
#先添加监听
ls -w [节点路径]
```

```shell
#然后在另外一个zkCli中给该节点添加子节点
create [节点路径]
```

##### acl权限控制列表

> 设置不同的服务器对zookeeper上节点数据的操作权限

>  使用：schema(模式)​ : id(用户) : ​permission(权限列表) 来标识 

###### 模式包括

| 方案   | 描述                                                | 对应的id       | 权限格式                |
| ------ | --------------------------------------------------- | -------------- | ----------------------- |
| world  | 只有一个用户：anyone，代表所有人（默认）            | 只有一个anyone | world:anyone:[权限列表] |
| ip     | 使用IP地址认证                                      | 目标的ip地址   | ip:[ip地址]:[权限列表]  |
| auth   | 使用已添加认证的用户认证, 就是明文用户名和密码      | 目标用户名     |                         |
| digest | 使用“用户名:密码”方式认证, 就是用户名和加密后的密码 | 目标用户名     |                         |

###### 权限列表

> 比如拥有所有权限表示为  cdrwa

| 权限   | ACL简写 | 描述                             |
| ------ | ------- | -------------------------------- |
| CREATE | c       | 可以创建子节点                   |
| DELETE | d       | 可以删除子节点（仅下一级节点）   |
| READ   | r       | 可以读取节点数据及显示子节点列表 |
| WRITE  | w       | 可以设置节点数据                 |
| ADMIN  | a       | 可以设置节点访问控制列表权限     |

###### 查看某节点权限列表

```shell
getAcl [节点路径]
```

![1606915458143](zookeeper.assets/1606915458143-1631955632230.png)

###### 设置权限列表

> 使用 setAcl <节点路径> <权限格式>

* 设置 world 模式的权限

    ```shell
    setAcl <节点路径> world:anyone:<权限列表>
    ```

* 设置 IP 模式的权限

    ```shell
    #设置赋予一个ip权限, 那么就只有该IP有权限
    setAcl <节点路径> ip:<IP地址>:<权限列表>
    #可同时设置多个ip权限,即同时设置多个权限规则
    setAcl <节点路径> ip:<IP地址>:<权限列表>,ip:<IP地址>:<权限列表>
    ```

* 设置 auth 模式的权限

    ```shell
    #首先得添加用户
    addauth digest <用户名>:<密码>
    
    #然后才能对该用户设置权限
    setAcl <节点路径> auth:<用户名>:<权限列表>
    ```

    ```shell
    #此时在别的机器上对该节点访问就需要先添加有权限的用户
    addauth digest <用户名>:<密码>
    #然后才能对该节点拥有对应的权限
    ```

* 设置 digest 模式的权限

    ```shell
    #首先使用linux命令生成用户名和密码的密文
    echo -n <用户名>:<密码> | openssl dgsl -binary -shal | openssl base64
    ```

    ```shell
    #同样是对用户的权限设置, 不同的是不需要先添加用户, 且使用加密后的密码
    setAcl <节点路径> digest:<用户名>:<上一步生成的密文>:<权限列表>
    #设置后需要添加对应用户才能拥有对应权限
    addauth digest <用户名>:<未经加密的密码>
    ```

* 同时设置多种权限模式, 使用 , 逗号分割多种权限格式

###### 设置内置的超管

> 为防止对节点设置了权限后忘了密码之类的尴尬, 应当设置一个内置的超级管理员账户, 通过修改 zkServer.bat 文件来实现, linux 下修改 zkServer.sh

1. 假设要添加的超管账号和密码是 super:admin, 还是先使用linux内置命令生成密文

    ```shell
    echo -n super:admin | openssl dgsl -binary -shal | openssl base64
    ```

2. 然后打开 zkServer 文件, 找到 `nohup $JAVA "-Dzookeeper.log.dir=${ZOO_LOG_DIR}" "-Dzookeeper.root.logger=${ZOO_LOG4J_PROP}"`, 在后面添加 `"-Dzookeeper.DigestAuthenticationProvider.superDigest=super:<上一步生成的密文>"`

3. 然后重启 zkServer

4. 然后添加 super 超管认证用户, 就拥有了所有节点的所有权限

    ```shell
    addauth digest super:admin
    ```

    

##### 集群部署

> 集群其实就是数据冗余, 所有节点都存一套数据

1. 整三个服务器, 使用同一台机器的三个不同端口也可以

2. 在各个节点的 zoo.cfg 文件中添加如下集群地址信息

    ```cfg
    server.1=<IP地址:端口:leader选举端口>
    server.2=<IP地址:端口:leader选举端口>
    server.3=<IP地址:端口:leader选举端口>
    ```

    如果是在一台机器的不同端口上开启服务, 记得修改 data 路径 和 端口号

3. 然后在各个节点的 data 目录下新建文件 myid , 编辑内容分别为 1,2,3

4. 分别使用 `zkServe` 启动服务

5. 使用 `zkServer.bat status` 检查启动状态

6. 使用 `zkCli.bat -server <IP地址:端口>` 可连接集群中指定的节点

> 集群中节点的几种类型:
>
> leader, fellower, observer

leader节点就相当于主节点, 写操作都会转发给 leader 节点来执行

fellower节点就相当于从节点

服务器启动时会进行 leader 选举, 就是谁的 myid 大谁就是 leader, 存在 leader 后就不再选举

observer节点不参与 leader 节点的选举, 不参与写数据时的ack反馈, 也就是写数据时不关心oberser节点写没写进去

配置一个 observer 节点:

1. 首先得有一个集群

2. 然后将各个节点的 zoo.cfg 配置文件将要作为 observer 节点的配置后面加上 :observer

    ```cfg
    server.1=<IP地址:端口:leader选举端口>
    server.2=<IP地址:端口:leader选举端口>
    #比如要用这台机器作为observer节点, 那么就在配置后面加上:observer
    server.3=<IP地址:端口:leader选举端口>:observer
    ```

3. 在 observer 节点的 zoo.cfg 中额外加上这样一句

    ```cfg
    peerType=observer
    ```

    

图形化工具

1、下载https://issues.apache.org/jira/secure/attachment/12436620/ZooInspector.zip

2、运行zookeeper-dev-ZooInspector.jar

  2.1 解压，进入目录ZooInspector\build

  2.2 在build目录，按住shift键右键鼠标，在右键菜单出选择“在此处打开命令窗口

  2.3 java -jar zookeeper-dev-ZooInspector.jar //执行成功后，会弹出java ui client

3、点击左上角连接按钮，输入zk服务地址：ip:2181
