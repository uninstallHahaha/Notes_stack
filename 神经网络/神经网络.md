# 神经网络

> 神经网络只是一个大概的结构的泛指, 即经过多层处理将原数据不断提取特征的过程
>
> 这里面每一层的操作并不是固定的 , 对于图片数据, 可以使用卷积操作和池化作为每一层中的提取特征的操作, 对于分类数据, 可以使用函数作为每一层中的操作  







# Tensorflow

> tf 的本质就是 拿着一堆矩阵 设置一系列的权重 进行一系列的运算

> pip install tensorflow
>
> pip install tensorflow-gpu 
>
> gpu版本还需安装 CUDA, CUDNN , 且版本必须匹配
>
> gpu版本使用gpu进行计算, gpu核心多但是都不强, 适合计算密集型程序
>
> gpu需要独显且3G以上显存
>
> cpu 核心较少 , 但是每一个核心都很强, 适合复杂计算型程序

#### tf 基本使用

> tf中的内容在 run 之前其实都是式子

```python
import numpy as np
#导包
import tensorflow as tf

#所有的 常量或变量 想要查看值 都需要在 session 中 run 

#常量
#常量都是 tensor 类型的对象, 直接调用返回的是一个对象并不是其中的值
a = tf.constant(1024)
b = tf.constant([1,2,3])
c = tf.constant('hello')
display(a,b,c)

#变量
#变量在 session 中 run 之前, 需要先进行全局变量初始化
a = np.random.randint(0,10,size=1)[0]
v = tf.Variable(initial_value = a)
#修改变量
v = tf.assign(v, 1000)

#占位符量, 同变量, 就是没数据
# shape中设置 None 就是任意高度
A = tf.placeholder(dtype=tf.float32, shape=[3,5])
B = tf.placeholder(dtype=tf.float32, shape=[5,None])
#没数据时如果 run 需要设置 feed_dict 来指定数据,否则报错
C = tf.matmul(A, B)
with tf.Session() as s:
    res = s.run(C, feed_dict={A:np.random.randint(0,10,size=(3,5)),
                             B:np.random.randint(0,10,size=(5,5))})
    print(res)



#运算, 运算时需要注意类型, float和int无法运算
a + b
#数的四则运算
tf.add(a,b)
tf.multipy(a,b)
#矩阵相乘 matrix multipy
a = tf.constant(np.random.randint(0,10,size=(3,2)), dtype=tf.float32)
b = tf.constant(np.random.randint(0,10,size=(2,4)), dtype=tf.float32)
c = tf.matmul(a,b)
c


#创建会话, 调用run方法返回传入的 表达式或常量 的值
s = tf.Session()
s.run(a) #返回常量a的值
s.run(tf.global_variables_initializer()) #run变量之前需要先全局变量初始化
s.run(v) #返回变量v的值

#关闭会话
s.close()

#或者直接使用 with 打开 session
with tf.Session() as s:
    pass


#tf中的常用方法
d = tf.reduce_mean(c, axis=1) #求平均值
```

#### tf 基本使用流程

1. 定义变量和占位符
2. 根据数学原理写方程
3. 定义损失函数 cost
4. 定义优化梯度下降 (GradientDescentOptimizer)
5. 在Session 中 for循环来训练参数
6. 保存 Saver



#### tf 反向传播

> tf  实际上也是在求一个函数模型 , 只不过求函数的方式是先瞎蒙一个, 然后通过不断地反向传播, 也就是调整参数来获取最优解
>
> https://www.cnblogs.com/charlotte77/p/5629865.html



#### tf 回归

###### tf  线性 回归

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.linear_model import LinearRegression
import tensorflow as tf

#生成大致为线性分布的数据
X_train = np.linspace(0,12,20).reshape(-1,1)
y_train = np.linspace(3,6,20) + np.random.randn(20)*0.4
plt.scatter(X_train, y_train)

#使用 sklearn的线性回归计算函数
lr = LinearRegression()
lr.fit(X_train, y_train)
plt.plot(np.linspace(0,12,100), lr.coef_*np.linspace(0,12,100)+lr.intercept_)

#使用 tf 线性回归
X = tf.placeholder(dtype=np.float32, shape=[20,1])
y = tf.placeholder(dtype=np.float32, shape=[20])
#随机给一个斜率值, tf中的random就是抄np的
#这里 w 就相当于一个矩阵, w和X相乘就相当于矩阵运算
w = tf.Variable(initial_value=tf.random_normal(shape=[1,1])) 
b = tf.Variable(initial_value=tf.random_normal(shape=[1,1])) 
#创建线性函数模型
pred = tf.matmul(X,w) + b
#最小二乘法求误差的平方的平均值 也就是损失值
cost = tf.reduce_mean((pred - tf.reshape(y, shape=[20,1]))**2)
#创建一个梯度下降计算器, 设置步幅为 0.01
gd = tf.train.GradientDescentOptimizer(0.01)
#使用梯度下降计算器对损失进行优化
optimizer = gd.minimize(cost)
optimizer
#循环进行梯度下降求最佳的 w 和 b
epoches = 1000 #设置梯度下降循环100次
with tf.Session() as s:
    s.run(tf.global_variables_initializer())
    for i in range(epoches):
        s.run(optimizer, feed_dict={X:X_train, y:y_train}) #梯度下降优化参数 w 和 b
        w_,b_ = s.run(fetches = [w,b]) #每次循环都获取并显示当前的 w 和 b
        print(w_, b_)
#然后会发现最后求得的 w 和 b 与sklearn中求出来的一样        
```

###### tf logistics 回归 : TODO

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import tensorflow as tf
#导入数据
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets('./', one_hot=True)
images = mnist.train.images #样本数据
images.shape
mnist.train.labels #样本目标值
plt.imshow(images[0].reshape(28,28))


X = tf.placeholder(dtype=tf.float32, shape=[None,784])
y = tf.placeholder(dtype=tf.float32, shape=[None,10])
W = tf.placeholder(dtype=tf.float32, shape=[None,10])



#softmax,就是将一个元素为float类型的数组中每个数都转换为概率, 转换之后所有的概率总和为 1
a = np.array([-1,2,3.0])
b = tf.nn.softmax(a)
with tf.Session() as s:
    print(s.run(b))
```



#### tf 卷积

> 卷积操作二维数据 ( 黑白图片 )

```python
#原数据 , 原数据可以使用图片数据 , 然后设置不同的卷积核对图片数据进行滤波等操作
data = np.array([[2,1,2,3,4],[1,2,3,1,3],[4,2,4,6,7],[6,4,3,3,1],[5,2,1,4,3]])
#卷积核
fllter = np.array([[-1,0,1],[-1,0,1],[-1,0,1]])
#转换原数据和卷积核转换为规定形状 和 float类型
input_data = tf.reshape(data, shape=[1,5,5,1])
input_data = tf.cast(input_data, dtype=tf.float32)
fllter = tf.cast(tf.reshape(fllter, shape=[3,3,1,1]), dtype=tf.float32)
#卷积操作
# input 设置原数据, 为四维的数据, 处理图片时, 对应含义为 [多少张图片, 图片的高, 图片的宽, 图片通道数]
# filter 设置卷积核, 为四维数据, 处理图片时, 对应的含义为 [卷积核的高, 卷积核的宽, 输入几个通道的数据, 最后输出几个通道的数据也就是有几个卷积核] 通用的含义 :[卷积核的高,卷积核的宽,输入的数据有多少层,输出的数据要多少层]
# padding 设置在卷积时是否填充一个边框, 使得卷积之后输出的结果形状和原数据形状相同, VALID随缘, SAME添加padding
conv = tf.nn.conv2d(input=input_data, filter=fllter, 
             strides=[1,1,1,1], padding='VALID', use_cudnn_on_gpu=False)
with tf.Session() as s:
    res = s.run(conv)
    print(res.reshape(3,3))
```

> 卷积操作三维数据 ( 彩色图片 )

```python
#读取图片数据
image = plt.imread('./xxx.jpg')
plt.imshow(image)
image.shape

#卷积核, 彩色图片三个通道对应三个二维卷积核, 所以卷积核就是三维的
#卷积核是 3*3 , 有 3 个卷积核, 每次从每个通道的每一个卷积位上获取1/27的值, 将三个通道上27个卷积位的值合到一起, 最后输出为一个通道
fllter = np.full(shape=[3,3,3], fill_value=1/27).reshape(3,3,3,1)
#创建卷积操作
conv = tf.nn.conv2d(input = image.reshape(1,图片的高, 图片的宽, 图片通道数).astype(np.float32),
            filter = fllter,
            strides = [1,1,1,1],
            padding = 'SAME')
#卷积
with tf.Session() as s:
    res = s.run(conv)
    plt.imshow(res.reshape(图片的高, 图片的宽), cmap = 'gray')
```



#### 神经网络

> 神经网络的本质就是一开始有几个初始的函数, 然后将它们随机线性组合和激活函数瞎蒙修改 , 看看结合出来的函数的图像能不能对问题进行分类, 然后根据误差调整线性组合时的权重, 一直循环直到误差很小 

##### 激活函数

> 其实就是将原数据经过一个函数运算, 然后取返回的数值
>
> 因为输入的函数就那么几个, 所以如果直接线性组合它们, 那么输出的结果就不能多样化, 所以激活函数的本质就是用另外一个函数对输出的结果再进行处理, 使得最后的结果更具多样性, 选用什么激活函数就是一个瞎蒙的过程

###### ReLu

```python
a = np.random.randint(-5,10,size=20)
b = tf.nn.relu(a)
with tf.Session() as s:    
    print(s.run(b))
```

##### dropout

> 样本量较小时, 可能发生过拟合, 此时就使用 dropout , 即屏蔽一部分的神经元, 使得输出的结果更具普适性 , 这个过程是一个瞎蒙的过程

##### tf 实现卷积神经网络

![1614047473308](神经网络.assets/1614047473308.png)

```python
#实现如上的卷积神经网络
#初始数据 32*32 
input_data = np.random.randn(32,32).reshape(1,32,32,1)

#卷积核 5*5*8
filter_ = np.random.randn(5,5,8).reshape(5,5,1,8)
#第一次卷积
conv = tf.nn.conv2d(input=input_data,filter=filter_,strides=[1,1,1,1],padding='VALID')
#第一次池化
pool = tf.nn.max_pool(conv, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')
#第一次激活
relu = tf.nn.relu(pool)
#第一次dropout, keep_prob 设置保留下来正常工作的神经元的比例
dropout = tf.nn.dropout(relu, keep_prob=0.6)

#第二次卷积, 上一层输出的结果有8层, 所以这里要有8个二维的卷积核
filter2 = np.random.randn(5,5,8,20)
conv2 = tf.nn.conv2d(input = dropout, filter=filter2,strides=[1,1,1,1],padding='VALID')
#池化
pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')
#激活
sigmoid = tf.nn.sigmoid(pool2)
#dropout
do2 = tf.nn.dropout(sigmoid, keep_prob=0.5)

#全连接 
#全连接其实就是将上一步输出的结果的所有的元素与一个与它个数相同的数组进行矩阵乘法, 最后返回一个数
#这里上一步的 do2 的形状是 1*5*5*20 , 那么它总共有 1*5*5*20 个元素, 那么就需要有 1*5*5*20 个系数分别与它们相乘然后加到一起
#这里使用 120套系数, 最后就返回120个结果 , 也就是返回一个形状为 1*120 的矩阵
#给这500个数随机生成 120 套系数
dense = np.random.randn(500,120)
#将上一步返回的结果排排站成一行
fc = tf.reshape(do2, shape=[1,1*5*5*20])
#进行所谓的全连接操作, 其实就是给它120套系数进行矩阵乘法, 最后返回 120 个数
conn = tf.matmul(fc, dense)
conn

#全连接之后再给它 9 套包含120个系数的矩阵进行 "全连接", 然后再加上一个 截距 b
#就是所谓的最终输出了 9 个特征
w = np.random.randn(120, 9)
b = np.random.randn(9)
out = tf.matmul(conn, w) + b
out
```



##### 卷积神经网络实现手写数字识别

> 数据样本集在此  https://blog.csdn.net/sinat_34715587/article/details/89228065

``` python
import numpy as np
import tensorflowa as tf
from tensorflow.examples.tutorials.mnist import input_data

#读取样本数据压缩文件 , 需要事先下载, 放到当前目录下
#one_hot 的读取方式可将数据转换成概率的形式
mnist = input_data.read_data_sets('./',one_hot=True)

#查看数据文件中的一张图片
img = mnist.train.images[0]
img.shape

#创建变量占位符
X = tf.placeholder(dtype=tf.float64, shape=[None, 样本图片的像素个数])
y = tf.placeholder(dtype=tf.float64, shape=[None, 10])

#卷积核在卷积神经网络中是变量, 因为要不断调整卷积核使得卷积结果不断接近真实值
#定义一个生成卷积核变量的方法
def gen_v(shape):
    #根据形状生成 方差为0.1且平局值为 0 的卷积核, 这样生成的卷积核中各个元素都接近于0
    # stddev 设置方差
    return tf.Variable(initial_value=tf.random_normal(dtype=tf.float64,shape=shape,stddev=0.1),
                      dtype=tf.float64)

#定义进行卷积操作的方法
def conv(input_data, filter_):
    return tf.nn.conv2d(input = input_data, filter=filter_,
                       strides=[1,1,1,1],padding='SAME')

#定义池化的方法
def pool(input_data):
    return tf.nn.max_pool(value=input_data, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')



###############################

#第一层卷积
#输入数据, 这里读取的数据是一维的784个像素一个通道的黑白图片, 所以这里需要转换为二维的数据
input_data = tf.reshape(X, shape=[-1,28,28,1])
#卷积核
filter1 = gen_v(shape=[3,3,1,64])
#偏差, 卷积之后生成 64 层 , 给每一层都加上一个随机的偏差
b1 = gen_v(shape=[64])
#卷积
conv1 = conv(input_data, filter1) + b1
#池化
pool1 = pool(conv1)
#激活函数
ac1 = tf.nn.relu(pool1)
ac1

#第二层卷积
filter2 = gen_v(shape=[3,3,64,64])
b2 = gen_v(shape=[64])
conv2 = conv(ac1, filter2) + b2
pool2 = pool(conv2)
ac2 = tf.nn.sigmoid(pool2)
ac2

#全连接
#这里使用 1024套参数对其进行全连接
fc_w = gen_v(shape=[7*7*64,1024])
fc_b = gen_v(shape=[1024])

conn = tf.matmul(tf.reshape(ac2, shape=[-1,7*7*64]), fc_w) + fc_b
conn

#dropout 防止过拟合
kp = tf.placeholder(dtype=tf.float64)
dropout = tf.nn.dropout(conn, keep_prob = kp)
dropout

#输出层out
#应当输出 10 个类别, 所以就整 10 套全连接参数
out_w = gen_v(shape=[1024,10])
out_b = gen_v(shape=[10])

out = tf.matmul(dropout, out_w) + out_b
out

#将10个类别转换成概率的形式
#目前这10个概率就是上面这些网络随机预测出来的概率
prob = tf.nn.sorfmax(out)
prob

#现在真实的概率是 y
y

#使用现有的 预测概率 和 真实概率 , 根据交叉熵公式求损失
cost = tf.reduce_mean(tf.reduce_sum(y*tf.log(1/prob),axis=-1))
cost

# 创建并使用优化算法
# 这个是基于梯度下降的一个优化算法
adam = tf.train.AdamOptimizer()
optimizer = adam.minimize(cost)
optimizer

#创建一个保存参数器
saver = tf.train.Saver()

# for 循环 run 上面的神经网络训练参数
with tf.Session() as s:
    s.run(tf.global_variables_initializer())
    #总共有55000条样本
    #优化10次
    #每一次优化都将 55000 样本分成 100 次计算损失, 最后将100次损失加到一起, 这样计算快些
    for i in range(10):
        c = 0
        for j in range(100):
            X_train , y_train = mnist.train.next_batch(550)
            optimizer_, cost_ = s.run(fetches=[optimizer, cost], 
                                      feed_dict={X:X_trian,y:y_train,kp:0.5})
            c += cost_/100
            print('里层执行次数: %d, 当次损失是: %0.4f' % (j,cost_))
            
        print('-----------执行次数: %d, 当前损失是: %0.4f--------------' % (i,c))
        #每次优化都保存参数, 保存路径需要事先创建
        saver.save(s, save_path='./cnn/model',global_step=i)
        
#以上代码运行完之后会在指定目录生成 10 个名字为 model-x 的模型参数文件
#加载最后一次生成的参数文件进行测试
with tf.Session() as s:
    saver.restore(s, './cnn/model-9')
    X_test, y_test = mnist.test.next_batch(2000)
    #代入测试数据到网络中, 求得预测出来的概率
    prob_ = s.run( prob, feed_dict={X:X_test,kp:1.0} )
    #求得真实值, 因为是 one_hot 的读取方式, 所以最大值的位置就是对应的真实值
    y_test = y_test.argmax(axis = -1)
    #求得预测的值
    prob_ = prob_.argmax(axis=-1)
    #比对下有多少个预测对的, 通过求平均值获取预测准确率 
    print((y_test == prob_).mean())
```



##### 卷积神经网络实现验证码识别 : 

> 样本数据文件夹名 Discuz

```python
import numpy as np
import cv2 as cv2
import tensorflow as tf


```



# Keras 

> 傻瓜版的 tensorflow , 将tf中创建神经网络的细节封装起来成为一些简单的 api

#### 图片数据集生成器

> 实际问题的处理中, 总不是能够获得到刚刚好大小和比例且图片主题在正中间的样本集
>
> 这个时候就可以使用图片生成器把不规整的图片都批量处理成为规整大小和比例的样本集

1. 将图片按照如下结构存放到文件系统中

   ![1614047512552](神经网络.assets/1614047512552.png)

2. 使用图片生成器

   ```python
   #导包
   from tensorflow.keras.preprocessing.image import ImageDataGenerator
   
   #生成器实例
   #设置将生成的数据除以 255 以标准化
   train_datagen = ImageDataGenerator(rescale = 1./255)
   # barch_size 一次生成多少张
   # class_mode 这是一个几分类问题
   # train_dir 源图片路径, 这个路径应当是包含图片目录的路径
   train_gen = train_datagen.flow_from_directory(train_dir, target_size=(300,300),
                                                batch_size = 128, class_mode='binary')
   
   #测试集数据生成, 同上
   test_datagen = ImageDataGenerator(rescale = 1./255)
   validation_gen = test_datagen.flow_from_directory(validation_dir, target_size=(300,300),
                                                batch_size = 32, class_mode='binary')
   
   ```

   





# TensorBoard

> tensorflow 自带的可视化工具
>
> 安装 TensorFlow 时就自动安装了这个

1. 在代码中生成网络的结构图文件

```python
import numpy as np
import tensorflow as tf

#定义一个名字为 xxx 的命名域, 这个域就是可视化界面中显示的一个模块
#然后在这个域中添加 常量 和 操作
#这里的域就相当于一个分组, 对于复杂的网络能够较好的理清关系. 当然也可以不创建这个域
#对于一个完整的网络, 应当以多个域分别给每一步操作进行分组
with tf.name_scope(name = 'xxx') as scope:
    # name 属性设置在结构图中显示的名字
    m1 = tf.constant(np.random.randint(0,10,size=(2,3)), name ='m1')
    m2 = tf.constant(np.random.randint(0,10,size=(3,5)), name ='m2')
    
    pro = tf.matmul(m1, m2, name = 'product')

sess = tf.Session()
#将指定 session 的结构图保存到指定文件夹下, 没有则自动创建
tf.summary.FileWriter('./logs', sess.gragh)
sess.run(pro)
sess.close()
```

2. 打开生成文件所在的路径下的 cmd

> 执行命令会开启一个web服务, 在 6006端口, 浏览器访问该端口可查看可视化界面

```shell
> tensorboard --logdir logs
```







# 附录

###### [反向传播](https://zhuanlan.zhihu.com/p/32819991)

在训练神经网络时, 先输入训练数据, 然后使用输出与真实结果计算 loss , 根据 loss 的期望方向对网络中的各个权重以及偏置参数进行调整.

假设网络的输出为 10个值, 那么想要改变这10个数的值就应当从 <span style="color:cyan;">与其相连的权重</span>, <span style="color:cyan;">偏置</span> 以及 <span style="color:cyan;">前一层的激活值</span> 下手, 

实际上应当从这三个方面同时下手进行调整, 才能最大化优化的速度. 

其中, <span style="color:cyan;">前一层的激活值</span>不能直接调整, 而是通过前一层神经元上的权重及其偏置下手进行调整, 因此, 从这一方面考虑调整时, 可以一直向前追溯每一层的三个调整方面, 从而对每一个神经元上的权重和偏置给出调整方向, 进而对其进行调整; 而对于 <span style="color:cyan;">与其相连的权重</span> 和 <span style="color:cyan;">偏置</span>, 则可以直接计算它们对 loss 值的偏导, 然后进行调整.

所以综上, 当网络接收一批训练数据后, 应当对这些数据计算 loss 后取平均, 这个平均 loss 就代表了兼顾当前输入所有数据的调整方向, 然后根据这个 loss  <span style="color:cyan;font-weight:bold;">更新网络中的所有参数</span>  达到优化的目的.





###### 随机梯度下降

理论上, 神经网络应当一次性接收所有的训练数据, 得到兼顾所有数据的正确的梯度, 然后进行调整, 但是实际上训练数据的海量导致如果一次性兼顾所有训练数据的话, 将会使得训练速度极慢.

所以, 将所有的数据打乱后分批输出网络多次进行优化才能较好的选择, 这种方式虽然不能保证每次优化的方向都是完全正确的, 但是所有批次的优化汇总到一起总会与正确的方向是一致的.

这种分批次进行优化的方式称为随机梯度下降, 这种方法极大地提高了训练的速度.




​	



###### sigmod函数

	全连接神经网络中 , 每个神经元的值都是由前一层的神经元乘以权重矩阵 , 然后加上bias矩阵, 最后经过调整函数来得到的, 早期的网络中多使用 sigmod 函数作为调整函数
	
	但是 sigmod 函数存在一个问题, 即仅仅在大概 -5 ~ 5 的范围内, 对应的值才有区分度, 超过了这个范围的值区分度微乎其微, 在这种情况下, 当对某个神经元前边的神经元求偏微分时, 可能因为该神经元本身的值超出了 -5 ~ 5 的范围而造成当前值的偏微分接近于0, 
	
	因为对底层的权重求梯度时使用链式法则, 所以在计算过程中必然包含了对 sigmod 的求导, 如果 sigmod 的导数很小, 那么将会导致最后求得的梯度极小, 那么此时底层的权重根据梯度进行的调整将微乎其微, 此时神经网络的梯度下降过程将接近停止, 使得网络失去优化的能力, 这种情况就是梯度消失
	
	为了改善 sigmod 函数带来的梯度消失问题, 现行做法一般是将该调整函数改为其他函数如 ReLu 





###### Nomalization

在实际训练网络时, 训练数据会分批输入到网络进行训练, 如果就直接给数据分批然后输入网络, 那么很可能这些批次的数据的分布都差的挺多的, 不能真实的反映整个训练数据集的分布, 网络优化的本质就是习得训练数据的分布, 这样就会导致网络的训练变的飘忽不定, 效果不好.

所以, 要在数据输入网络之前, 进行规范化, 使其能够反映整个数据集的分布, 这样的操作就是 Nomalization , 这个操作用在输入层	







###### [BN](https://www.cnblogs.com/eilearn/p/9780696.html)

> Batch normalization

	BN是用在隐藏层神经元上的操作
	
	在训练神经网络时, 如果某次给定的一批训练数据中有噪声数据, 也就是跟其他大部分数据的分布差很远的那种, 不合群的那种, 跟大体的数据分布范围差了八竿子远的那种.
	
	因为网络在本次优化时要兼顾考虑这一批数据中的所有项, 所以这不合理的数据也被考虑进来了, 就会导致优化的方向就那么的不太好,

这种现象被称为  Internal Covariate Shift 

	正则化操作就是把所有的数据都拉到一个较小范围内, 这样就避免了那种不合群的数据对网络优化造成的骚扰.
	
	BN 的操作, 就是求数据的 mean , 求数据的 various , 然后根据这两个值把原数据换算到这个圆形分布上
	
	下图为BN的操作公式, 把数据分布想象到二维平面上, 先用一个圆 $C_1$ 把数据大致框起来, mean 大致为圆心位置, various 大致为 $C_1$ 半径的一半 

![1617265817393](神经网络.assets/1617265817393.png)

BN的好处 :

1. <span style='color:cyan;'>初始化</span>网络的时候不需要那么谨慎了

   	在常规的网络中, 可以看做数据每经过一层神经元, 就被映射为另外一个分布, 如果数据经过某层神经元映射出来的分布包含哪种放飞自我的不合群的分布点, 那么就把它后面层的网络害了, 因为后面层的网络要兼顾这些不合群的点计算梯度反向传播优化网络
		
      	如果在每一层网络的输出后面都整上一层 正则化 操作, 那么网络中将不会再出现不合群的分布点, 也就提高了网络训练的稳定度

2. <span style='color:cyan;'>学习率</span>的设定也不用那么谨慎了

   	同上, 在常规的网络中不加正则的情况下, 可能搞出来不合群的分布点, 这个时候带来的优化方向就是不怎么样的, 如果这个时候徐诶率也恰好很大, 那么就是在往错误的方向走了一大步, 搞得优化操作变成了南辕北辙, 白忙活半天了

3. <span style='color:cyan;'>减少dropout的使用</span>

   	dropout的使用是为了降低过拟合的风险, 因为BN是对分布进行了调整, 所以使用BN后本身就不再是对数据本身进行训练, 所以使用BN后提高了模型的泛化能力

4. <span style='color:cyan;'>解决了反向传播过程中梯度消失的问题</span>

   	因为底层的权重是经过多个sigmod求的导数, 所以很容易导致梯度消失, 使用BN后, 数据分布集中在 sigmod 的高效区域, 就不会再造成梯度消失的问题







[Dropout](https://zhuanlan.zhihu.com/p/38200980)





RNN -> LSTM -> transformer





[LSTM](https://zhuanlan.zhihu.com/p/32085405)

